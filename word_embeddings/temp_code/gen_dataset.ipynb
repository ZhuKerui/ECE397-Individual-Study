{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd0947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from tools.DocProcessing.CoOccurrence import co_occur_load\n",
    "from tools.BasicUtils import MultiProcessing, my_read, my_write\n",
    "from tools.DocProcessing.CoOccurGraph import graph_load, get_subgraph\n",
    "# from tools.DocProcessing.DatasetGenerator import TriEntities\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = my_read('../data/corpus/entity.txt')\n",
    "co_occur_list = co_occur_load('../data/corpus/ent_co_occur.txt')\n",
    "pair_graph = get_subgraph(graph_load('../data/corpus/ent_pair.gpickle'), 0.3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import networkx as nx\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "class TriEntities:\n",
    "    def __init__(self, entity_list:List[str], co_occur_list:List[List[int]], pair_graph:nx.Graph, kw_dist_max:int=6, sent_length_max:int=32):\n",
    "        self.keyword_list = entity_list\n",
    "        self.keyword_set = set(entity_list)\n",
    "        self.pair_graph = pair_graph\n",
    "        self.kw_dist_max = kw_dist_max\n",
    "        self.sent_length_max = sent_length_max\n",
    "        self.co_occur_list = co_occur_list\n",
    "        self.line_record = []\n",
    "\n",
    "    def line_operation(self, line:str):\n",
    "        line_id, sent = line.split(':', 1)\n",
    "        tokens = sent.split()\n",
    "        if len(tokens) > self.sent_length_max:\n",
    "            return\n",
    "        kws = self.co_occur_list[int(line_id)-1]\n",
    "        kws = [idx for idx in kws if tokens.count(self.keyword_list[idx]) == 1]\n",
    "        if len(kws) < 3:\n",
    "            return\n",
    "        pairs = [(self.keyword_list[kws[i]], self.keyword_list[kws[j]]) for i in range(len(kws)-1) for j in range(i+1, len(kws)) if self.pair_graph.has_edge(kws[i], kws[j])]\n",
    "        pairs = [pair for pair in pairs if abs(tokens.index(pair[0]) - tokens.index(pair[1])) <= self.kw_dist_max]\n",
    "        if len(pairs) < 2:\n",
    "            return\n",
    "        kws = [self.keyword_list[idx] for idx in kws]\n",
    "        temp_list = []\n",
    "        for kw in kws:\n",
    "            pair_idx = [1 if kw in pair else 0 for pair in pairs]\n",
    "            if sum(pair_idx) < 2:\n",
    "                continue\n",
    "            kw_set = set()\n",
    "            sub_pairs = [pair for i, pair in enumerate(pairs) if pair_idx[i] == 1]\n",
    "            for pair in sub_pairs:\n",
    "                kw_set.update(pair)\n",
    "            kw_set.remove(kw)\n",
    "            temp_list.append((kw, kw_set))\n",
    "        if not temp_list:\n",
    "            return\n",
    "        \n",
    "        doc = nlp(sent)\n",
    "        tokens = [token.text for token in doc]\n",
    "        kws = [kw for kw in kws if kw in tokens]\n",
    "        for media_kw, kw_set in temp_list:\n",
    "            subj_test = False\n",
    "            subj_text = ''\n",
    "            obj_test = False\n",
    "            obj_text = ''\n",
    "            for kw in kw_set:\n",
    "                if kw not in kws:\n",
    "                    continue\n",
    "                if doc[tokens.index(kw)].dep_ == 'nsubj':\n",
    "                    subj_test = True\n",
    "                    subj_text = kw\n",
    "                elif doc[tokens.index(kw)].dep_ == 'dobj':\n",
    "                    obj_test = True\n",
    "                    obj_text = kw\n",
    "            if subj_test and obj_test:\n",
    "                self.line_record.append((sent, media_kw, subj_text, obj_text))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mark_sent_in_html(sent:str, keyword_list:List[str], is_entity:bool=True):\n",
    "    reformed_sent = sent.split() if is_entity else sent_lemmatize(sent.replace('-', ' - '))\n",
    "    reformed_keywords = [[k] for k in keyword_list] if is_entity else [k.replace('-', ' - ').split() for k in keyword_list]\n",
    "    mask = np.zeros(len(reformed_sent), dtype=np.bool)\n",
    "    for k in reformed_keywords:\n",
    "        begin_idx = 0\n",
    "        while reformed_sent[begin_idx:].count(k[0]) > 0:\n",
    "            begin_idx = reformed_sent.index(k[0], begin_idx)\n",
    "            is_good = True\n",
    "            i = 0\n",
    "            for i in range(1, len(k)):\n",
    "                if begin_idx + i >= len(reformed_sent) or reformed_sent[begin_idx + i] != k[i]:\n",
    "                    is_good = False\n",
    "                    break\n",
    "            if is_good:\n",
    "                mask[begin_idx:begin_idx+i+1] = True\n",
    "            begin_idx += (i+1)\n",
    "    i = 0\n",
    "    insert_idx = 0\n",
    "    while i < len(mask):\n",
    "        if mask[i] and (i == 0 or mask[i-1] == False):\n",
    "            reformed_sent.insert(insert_idx, '<font style=\\\"color:red;\\\">')\n",
    "            insert_idx += 2\n",
    "            i += 1\n",
    "            while i < len(mask) and mask[i]:\n",
    "                i += 1\n",
    "                insert_idx += 1\n",
    "            reformed_sent.insert(insert_idx, '</font>')\n",
    "            insert_idx += 1\n",
    "        insert_idx += 1\n",
    "        i += 1\n",
    "    return ' '.join(reformed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n"
     ]
    }
   ],
   "source": [
    "te = TriEntities(keyword_list, co_occur_list, pair_graph)\n",
    "for idx, line in enumerate(open('../data/corpus/small_sent_line.txt').readlines()):\n",
    "    te.line_operation(line)\n",
    "    if idx % 100000 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1135"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "len(te.line_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "for sent, media_kw, subj_text, obj_text in te.line_record:\n",
    "    content.append('<h3>subject: %s, object: %s, connection: %s</h3><br>' % (subj_text, obj_text, media_kw))\n",
    "    content.append('%s<br><br>' % mark_sent_in_html(sent, [subj_text, obj_text, media_kw]))\n",
    "my_write('triples.html', content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(te.line_record, columns=['sent', 'media_kw', 'connected_kw'])\n",
    "df.to_csv('../data/corpus/connected_kws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = df.sample(frac=1).reset_index(drop=True)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'python' in set(test_lines['media_kw'])"
   ]
  }
 ]
}