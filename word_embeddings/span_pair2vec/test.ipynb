{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "metadata": {
    "interpreter": {
     "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded checkpoint 'data/result_3/best_model.pt' (epoch 17 iter: 540001 train_loss: 1.632509862942454, dev_loss: 2.3585431747436525, train_pos:0.5035273432731628, train_neg: 0.017596999183297157, dev_pos: 0.31248798966407776, dev_neg: 0.016466999426484108)\n"
     ]
    }
   ],
   "source": [
    "from embeddings.play import Play\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from my_util import ugly_normalize, ntopidx\n",
    "from vdbscan import do_cluster\n",
    "\n",
    "d = Play('data/result_3/best_model.pt', 'data/result_3/saved_config.json', 'data/rel_20.txt')\n",
    "keywords = open('data/keyword_p.txt').read().strip().split('\\n')\n",
    "keywords_token, keywords = d.token_align(keywords, 6)\n",
    "relations = open('data/rel_20.txt').read().strip().split('\\n')\n",
    "relations_token = [rel.split() for rel in relations]\n",
    "relations = [(rel[:rel.index('<pad>')].strip() if '<pad>' in rel else rel) for rel in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for finding similar keywords and relations\n",
    "def read_test_file(file_name:str):\n",
    "    ret = []\n",
    "    with open(file_name) as f_in:\n",
    "        for line in f_in:\n",
    "            central_kw, kws = line.strip().split(':')\n",
    "            ret.append((central_kw, kws.split(',')))\n",
    "    return ret\n",
    "\n",
    "def run_test(test_data:list, d:Play, keywords:list, keywords_token:list, relations:list):\n",
    "    ret = []\n",
    "    for central_kw, kws in test_data:\n",
    "        central_kw_token = central_kw.split()\n",
    "        kws_token, kws = d.token_align(kws, 6)\n",
    "        general_rel_prediction = d.get_prediction(keywords_token, [central_kw_token] * len(keywords_token))\n",
    "        test_rel_prediction = d.get_prediction(kws_token, [central_kw_token] * len(kws_token))\n",
    "        rel_score = cosine_similarity(test_rel_prediction, d.relation_representation)\n",
    "        rel_predict_score = cosine_similarity(test_rel_prediction, general_rel_prediction)\n",
    "        for i in range(len(kws)):\n",
    "            rel_top_40 = [relations[idx] for idx in ntopidx(40, rel_score[i])]\n",
    "            kws_top_40 = [keywords[idx] for idx in ntopidx(40, rel_predict_score[i])]\n",
    "            ret.append((kws[i], central_kw, rel_top_40, kws_top_40))\n",
    "    return ret\n",
    "\n",
    "def write_result(data:list, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f_out:\n",
    "        content = []\n",
    "        for kw, central_kw, similar_rels, similar_kws in data:\n",
    "            content.append('!%s<=>%s\\n' % (kw, central_kw))\n",
    "            content.append('>Similar Relation')\n",
    "            content += similar_rels\n",
    "            content.append('\\n>Similar Keyword')\n",
    "            content += similar_kws\n",
    "            content.append('\\n')\n",
    "        f_out.write('\\n'.join(content))\n",
    "\n",
    "def do_test(test_file:str, result_file:str, d:Play, keywords:list, keywords_token:list, relations:list):\n",
    "    test_data = read_test_file(test_file)\n",
    "    result = run_test(test_data, d, keywords, keywords_token, relations)\n",
    "    write_result(result, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for doing relation clustering\n",
    "def find_rel(target_rel:str, rel_list:list) -> list:\n",
    "    ret = []\n",
    "    for i, line in enumerate(rel_list):\n",
    "        if target_rel in line:\n",
    "            ret.append((line, i))\n",
    "    return ret\n",
    "\n",
    "def find_group_member(target:int, clusters:dict):\n",
    "    for value in clusters.values():\n",
    "        if target in value:\n",
    "            return value\n",
    "\n",
    "def find_most_similar(target:int, vecs:np.ndarray, relation_representation:np.ndarray, n:int=10):\n",
    "    target_vec = relation_representation[vecs]\n",
    "    similarities = cosine_similarity(target_vec, vecs)\n",
    "    return ntopidx(n, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test('test/single_word_test.txt', 'result/single_word_out_2.txt', d, keywords, keywords_token, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c1dbc394ac4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcluster_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgroup_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/similar_relation/project/word_embeddings/vdbscan.py\u001b[0m in \u001b[0;36mdo_cluster\u001b[0;34m(dataset, k)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# plot_avg_dist(avg_dist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mslope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_slope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mEps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_paras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mnoiseList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcluster_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/similar_relation/project/word_embeddings/vdbscan.py\u001b[0m in \u001b[0;36mgenerate_paras\u001b[0;34m(dist_matrix, slope, avg_dist, interval)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mslopeDiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtemp_Minpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_minpts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslopeDiff\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mslopeDiff\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemp_Minpts\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mEps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/similar_relation/project/word_embeddings/vdbscan.py\u001b[0m in \u001b[0;36mget_minpts\u001b[0;34m(dist_matrix, eps)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_minpts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mfind_neighbor_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdist_matrix\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfind_neighbor_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfind_neighbor_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate clusters\n",
    "k = 7\n",
    "cluster_id = do_cluster(d.relation_representation, k)\n",
    "group_num = max(cluster_id) + 1\n",
    "rel_id = np.arange(len(cluster_id))\n",
    "rel_clusters = {}\n",
    "for cid in range(-1, group_num):\n",
    "    rel_clusters[cid] = set(rel_id[cluster_id == cid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "temp = DBSCAN(eps=0.1 , min_samples=5, metric='cosine').fit(d.relation_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "(temp.labels_== 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "907"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "max(temp.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "s = np.arange(len(temp.labels_))[temp.labels_ == 102]\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "are based on\nhowever are based on\nwere based on\nare often based on\nare mainly based on\nare mostly based on\nare either based on\nare usually based on\nare proposed based on\nare derived based on\nare designed based on\nare typically based on\ncurrently are based on\nare based upon\nare selected based on\nare extracted based on\nhowever are mainly based on\nare largely based on\nare primarily based on\nare identified based on\nare generated based on\nare determined based on\nmoreover are based on\nhence are based on\nare still based on\nare generally based on\nare constraints based on\nare based primarily on\ntraditionally are based on\nare currently based on\nare analysed based on\nare reported based on\nare therefore based on\nare therefore often based on\nmoreover are based primarily on\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([relations[i] for i in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}