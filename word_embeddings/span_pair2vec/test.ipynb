{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "metadata": {
    "interpreter": {
     "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded checkpoint 'data/result_3/best_model.pt' (epoch 17 iter: 540001 train_loss: 1.632509862942454, dev_loss: 2.3585431747436525, train_pos:0.5035273432731628, train_neg: 0.017596999183297157, dev_pos: 0.31248798966407776, dev_neg: 0.016466999426484108)\n"
     ]
    }
   ],
   "source": [
    "from embeddings.demo import Demo\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "\n",
    "def token_align(pharses:list, length:int):\n",
    "    tokens = [pharse.split() for pharse in pharses if len(pharse.split()) <= length]\n",
    "    return [token + ['<pad>'] * (length - len(token)) for token in tokens], [' '.join(token) for token in tokens]\n",
    "\n",
    "d = Demo('data/result_3/best_model.pt', 'data/result_3/saved_config.json', 'data/rel_20.txt')\n",
    "keywords = open('data/keyword_p.txt').read().strip().split('\\n')\n",
    "keywords_token, keywords = token_align(keywords, 6)\n",
    "relations = open('data/rel_20.txt').read().strip().split('\\n')\n",
    "relations_token = [rel.split() for rel in relations]\n",
    "relations = [(rel[:rel.index('<pad>')].strip() if '<pad>' in rel else rel) for rel in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ugly_normalize(vecs:np.ndarray):\n",
    "   normalizers = np.sqrt((vecs * vecs).sum(axis=1))\n",
    "   normalizers[normalizers==0]=1\n",
    "   return (vecs.T / normalizers).T\n",
    "\n",
    "def ntopidx(n, score:np.ndarray):\n",
    "    s = nlargest(n, zip(np.arange(len(score)), score), key = lambda x: x[1])\n",
    "    return [item[0] for item in s]\n",
    "\n",
    "def read_test_file(file_name:str):\n",
    "    ret = []\n",
    "    with open(file_name) as f_in:\n",
    "        for line in f_in:\n",
    "            central_kw, kws = line.strip().split(':')\n",
    "            ret.append((central_kw, kws.split(',')))\n",
    "    return ret\n",
    "\n",
    "def run_test(test_data:list, d:Demo, keywords:list, keywords_token:list, relations:list):\n",
    "    ret = []\n",
    "    for central_kw, kws in test_data:\n",
    "        central_kw_token = central_kw.split()\n",
    "        kws_token, kws = token_align(kws, 6)\n",
    "        general_rel_prediction = ugly_normalize(d.model.get_prediction(keywords_token, [central_kw_token] * len(keywords_token)).numpy())\n",
    "        test_rel_prediction = ugly_normalize(d.model.get_prediction(kws_token, [central_kw_token] * len(kws_token)).numpy())\n",
    "        rel_score = np.matmul(test_rel_prediction, d.relation_representation.T)\n",
    "        rel_predict_score = np.matmul(test_rel_prediction, general_rel_prediction.T)\n",
    "        for i in range(len(kws)):\n",
    "            rel_top_40 = [relations[idx] for idx in ntopidx(40, rel_score[i])]\n",
    "            kws_top_40 = [keywords[idx] for idx in ntopidx(40, rel_predict_score[i])]\n",
    "            ret.append((kws[i], central_kw, rel_top_40, kws_top_40))\n",
    "    return ret\n",
    "\n",
    "def write_result(data:list, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f_out:\n",
    "        content = []\n",
    "        for kw, central_kw, similar_rels, similar_kws in data:\n",
    "            content.append('!%s<=>%s\\n' % (kw, central_kw))\n",
    "            content.append('>Similar Relation')\n",
    "            content += similar_rels\n",
    "            content.append('\\n>Similar Keyword')\n",
    "            content += similar_kws\n",
    "            content.append('\\n')\n",
    "        f_out.write('\\n'.join(content))\n",
    "\n",
    "def do_test(test_file:str, result_file:str, d:Demo, keywords:list, keywords_token:list, relations:list):\n",
    "    test_data = read_test_file(test_file)\n",
    "    result = run_test(test_data, d, keywords, keywords_token, relations)\n",
    "    write_result(result, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test('test/single_word_test.txt', 'result/single_word_out.txt', d, keywords, keywords_token, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}