{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "metadata": {
    "interpreter": {
     "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from span_pair2vec.embeddings.play import Play\n",
    "# from embeddings.play import Play\n",
    "# import numpy as np\n",
    "# from heapq import nlargest\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from my_util import ntopidx\n",
    "# from vdbscan import do_cluster\n",
    "\n",
    "# d = Play('data/result_3/best_model.pt', 'data/result_3/saved_config.json', 'data/rel_20.txt')\n",
    "# keywords = open('data/keyword_p.txt').read().strip().split('\\n')\n",
    "# keywords_token, keywords = d.token_align(keywords, 6)\n",
    "# relations = open('data/rel_20.txt').read().strip().split('\\n')\n",
    "# relations_token = [rel.split() for rel in relations]\n",
    "# relations = [(rel[:rel.index('<pad>')].strip() if '<pad>' in rel else rel) for rel in relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for finding similar keywords and relations\n",
    "def read_test_file(file_name:str):\n",
    "    ret = []\n",
    "    with open(file_name) as f_in:\n",
    "        for line in f_in:\n",
    "            central_kw, kws = line.strip().split(':')\n",
    "            ret.append((central_kw, kws.split(',')))\n",
    "    return ret\n",
    "\n",
    "def run_test(test_data:list, d:Play, keywords:list, keywords_token:list, relations:list):\n",
    "    ret = []\n",
    "    for central_kw, kws in test_data:\n",
    "        central_kw_token = central_kw.split()\n",
    "        kws_token, kws = d.token_align(kws, 6)\n",
    "        general_rel_prediction = d.get_prediction(keywords_token, [central_kw_token] * len(keywords_token))\n",
    "        test_rel_prediction = d.get_prediction(kws_token, [central_kw_token] * len(kws_token))\n",
    "        rel_score = cosine_similarity(test_rel_prediction, d.relation_representation)\n",
    "        rel_predict_score = cosine_similarity(test_rel_prediction, general_rel_prediction)\n",
    "        for i in range(len(kws)):\n",
    "            rel_top_40 = [relations[idx] for idx in ntopidx(40, rel_score[i])]\n",
    "            kws_top_40 = [keywords[idx] for idx in ntopidx(80, rel_predict_score[i])]\n",
    "            ret.append((kws[i], central_kw, rel_top_40, kws_top_40))\n",
    "    return ret\n",
    "\n",
    "def write_result(data:list, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f_out:\n",
    "        content = []\n",
    "        for kw, central_kw, similar_rels, similar_kws in data:\n",
    "            content.append('!%s<=>%s\\n' % (kw, central_kw))\n",
    "            content.append('>Similar Relation')\n",
    "            content += similar_rels\n",
    "            content.append('\\n>Similar Keyword')\n",
    "            content += similar_kws\n",
    "            content.append('\\n')\n",
    "        f_out.write('\\n'.join(content))\n",
    "\n",
    "def do_test(test_file:str, result_file:str, d:Play, keywords:list, keywords_token:list, relations:list):\n",
    "    test_data = read_test_file(test_file)\n",
    "    result = run_test(test_data, d, keywords, keywords_token, relations)\n",
    "    write_result(result, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for doing relation clustering\n",
    "def find_rel(target_rel:str, rel_list:list) -> list:\n",
    "    ret = []\n",
    "    for i, line in enumerate(rel_list):\n",
    "        if target_rel in line:\n",
    "            ret.append((line, i))\n",
    "    return ret\n",
    "\n",
    "def find_group_member(target:int, clusters:dict):\n",
    "    for value in clusters.values():\n",
    "        if target in value:\n",
    "            return value\n",
    "\n",
    "def find_most_similar(target:int, vecs:np.ndarray, relation_representation:np.ndarray, n:int=10):\n",
    "    target_vec = relation_representation[vecs]\n",
    "    similarities = cosine_similarity(target_vec, vecs)\n",
    "    return ntopidx(n, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_test('test/non_overlap_test.txt', 'result/non_overlap_out_2.txt', d, keywords, keywords_token, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clusters\n",
    "k = 7\n",
    "cluster_id = do_cluster(d.relation_representation, k)\n",
    "group_num = max(cluster_id) + 1\n",
    "rel_id = np.arange(len(cluster_id))\n",
    "rel_clusters = {}\n",
    "for cid in range(-1, group_num):\n",
    "    rel_clusters[cid] = set(rel_id[cluster_id == cid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "temp = DBSCAN(eps=0.1 , min_samples=5, metric='cosine').fit(d.relation_representation)\n",
    "with open('result/relation_cluster.txt', 'w', encoding='utf-8') as f_out:\n",
    "    content = []\n",
    "    for i in range(max(temp.labels_)):\n",
    "        content.append(str(i))\n",
    "        s = np.arange(len(temp.labels_))[temp.labels_ == i]\n",
    "        content += [relations[i] for i in s]\n",
    "        content.append('')\n",
    "    f_out.write('\\n'.join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_keywords import Keyword_Base\n",
    "\n",
    "kb = Keyword_Base()\n",
    "kb.load_word_tree('data/wordtree.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_triple(wordtree:dict, line:str):\n",
    "    if not line:\n",
    "        return False\n",
    "    items = line.split(';')\n",
    "    if len(items) != 3:\n",
    "        return False\n",
    "    if len(items[1].split()) > 6:\n",
    "        return False\n",
    "    for phrase in (items[0], items[2]):\n",
    "        word_tokens = phrase.split()\n",
    "        if len(word_tokens) > 6:\n",
    "            return False\n",
    "        i = 0\n",
    "        keyword_num = 0\n",
    "        has_key = False\n",
    "        while i < len(word_tokens):\n",
    "            if word_tokens[i] not in wordtree.keys():\n",
    "                i += 1\n",
    "            else:\n",
    "                # If the word is the start of a key word\n",
    "                it = wordtree\n",
    "                while i < len(word_tokens) and word_tokens[i] in it.keys():\n",
    "                    if \"\" in it[word_tokens[i]].keys():\n",
    "                        # If the word could be the last word of a keyword, the pharse fulfills requirement\n",
    "                        has_key = True\n",
    "                        break\n",
    "                    # Go down the tree to the next child\n",
    "                    it = it[word_tokens[i]]\n",
    "                    i += 1\n",
    "            if has_key:\n",
    "                break\n",
    "        if not has_key:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ollie_triple.txt') as f_in:\n",
    "    content = []\n",
    "    for line in f_in:\n",
    "        line = line.lower().strip()\n",
    "        if process_triple(kb.MyTree, line):\n",
    "            content.append(line)\n",
    "    with open('data/ollie_triple_f.txt', 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write('\\n'.join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for finding similar keywords\n",
    "def find_similar_keywords(test_data:list, d:Play, keywords:list, keywords_token:list):\n",
    "    ret = []\n",
    "    kws_token, kws = d.token_align(test_data, 6)\n",
    "    general_keywords_representation = d.encode_keyword_phrase(keywords_token)\n",
    "    test_keywords_representation = d.encode_keyword_phrase(kws_token)\n",
    "    score = cosine_similarity(test_keywords_representation, general_keywords_representation)\n",
    "    for i in range(len(kws)):\n",
    "        kws_top_40 = [keywords[idx] for idx in ntopidx(40, score[i])]\n",
    "        ret.append((kws[i], kws_top_40))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/similar_keywords.txt', 'w', encoding='utf-8') as f_out:\n",
    "    test_data = ['binary search tree', 'linked list', 'hash table', 'natural language processing', 'computer vision', 'speech recognition', 'binary search algorithm', 'linear regression', 'logistic regression', 'machine learning']\n",
    "    similar_keywords = find_similar_keywords(test_data, d, keywords, keywords_token)\n",
    "    content = []\n",
    "    for kw, similar_kws in similar_keywords:\n",
    "        content.append(kw)\n",
    "        content.append('')\n",
    "        content += similar_kws\n",
    "        content.append('')\n",
    "    f_out.write('\\n'.join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}