{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd0947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from my_keywords import Keyword_Base\n",
    "from my_multithread import MultiThreading\n",
    "from my_util import lemmatize_all, my_read, my_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process keyword file\n",
    "\n",
    "filtered_words = set(['can', 'it', 'work', 'in', 'parts', 'its', 'a','b','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "with open('../data/raw_data/keyword.csv', 'r') as f_in:\n",
    "    for line in f_in:\n",
    "        kw = line.split(',')[0]\n",
    "        if '- ' in kw:\n",
    "            continue\n",
    "        splited = kw.replace('-', ' - ')\n",
    "        reformed = lemmatize_all(splited)\n",
    "        if reformed in filtered_words:\n",
    "            continue\n",
    "        if reformed == splited:\n",
    "            stable_kw.append(kw)\n",
    "        else:\n",
    "            unstable_kw.append('%s\\t%s' % (kw, reformed))\n",
    "\n",
    "with open('../data/corpus/keyword_f.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(stable_kw))\n",
    "\n",
    "with open('../data/log/unstable_keyword.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(unstable_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building word tree is accomplished with 72102 words added\n"
     ]
    }
   ],
   "source": [
    "# Generate word tree\n",
    "keyword_base = Keyword_Base()\n",
    "keyword_base.build_word_tree('../data/corpus/keyword_f.txt', '../data/corpus/wordtree.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentence from small corpus (2 minute)\n",
    "!sed -n '/^ *\"abstract/p' ../data/raw_data/small_arxiv.json | sed 's/.*\\\"abstract\\\": \\\"  \\(.*\\)\\\\n\\\",.*/\\1/;s/\\\\n/ /g;s/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/small_sent.txt\n",
    "\n",
    "python ../my_sent_tokenize.py ../data/corpus/small_sent.txt ../data/corpus/small_sent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentence from big corpus\n",
    "# def extract_sent(line:str):\n",
    "#     if not line:\n",
    "#         return None\n",
    "#     jsonObj = json.loads(line)\n",
    "#     para = jsonObj['abstract'].strip().replace('\\n', ' ').replace('$', '').replace('--', ', ').replace('-', ' - ')\n",
    "#     return para + '\\n'\n",
    "# multithreading = MultiThreading()\n",
    "# content = multithreading.run(extract_sent, input_file='../data/raw_data/big_arxiv.json', output_file='../data/corpus/big_sent.txt', thread_num=20).split('\\n')"
   ]
  }
 ]
}