{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd0947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_write, MultiProcessing\n",
    "from tools.TextProcessing import Occurance, occurance_dump, occurance_load, occurance_post_operation, sent_lemmatize\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Co-occurance list related functions\n",
    "def gen_co_occur(occur_dict:Dict[str, set], sent_len:int, word2idx:Dict[str, int]):\n",
    "    co_occur_list = [set() for i in range(sent_len)]\n",
    "    for key, set_ in occur_dict.items():\n",
    "        idx = word2idx[key]\n",
    "        for line in set_:\n",
    "            co_occur_list[line].add(idx)\n",
    "    return co_occur_list\n",
    "\n",
    "def co_occur_dump(co_occur_file:str, co_occur_list:List[set]):\n",
    "    my_write(co_occur_file, [' '.join(map(str, set_)) for set_ in co_occur_list])\n",
    "\n",
    "def co_occur_load(co_occur_file:str):\n",
    "    return [list(map(int, line.split())) for line in my_read(co_occur_file)]\n",
    "\n",
    "# NPMI related functions\n",
    "def build_graph(co_occur_list:List[List[int]], keywords:List[str]):\n",
    "    g = nx.Graph(c=0)\n",
    "    g.add_nodes_from(range(len(keywords)), c=0)\n",
    "    print('Reading Co-occurance lines')\n",
    "    for line_idx, line in enumerate(co_occur_list):\n",
    "        kw_num = len(line)\n",
    "        g.graph['c'] += kw_num * (kw_num - 1)\n",
    "        for i in range(kw_num):\n",
    "            u = line[i]\n",
    "            g.nodes[u]['c'] += (kw_num - 1)\n",
    "            for j in range(i+1, kw_num):\n",
    "                v = line[j]\n",
    "                if not g.has_edge(u, v):\n",
    "                    g.add_edge(u, v, c=0)\n",
    "                g.edges[u, v]['c'] += 1\n",
    "        if line_idx % 5000 == 0:\n",
    "            print('\\r%d' % line_idx, end='')\n",
    "    print('')\n",
    "    print('Reading Done! NPMI analysis starts...')\n",
    "    Z = float(g.graph['c'])\n",
    "    for e, attr in g.edges.items():\n",
    "        attr['npmi'] = -math.log((2 * Z * attr['c']) / (g.nodes[e[0]]['c'] * g.nodes[e[1]]['c'])) / math.log(2 * attr['c'] / Z)\n",
    "    print('NPMI analysis Done')\n",
    "    return g\n",
    "\n",
    "def graph_dump(g:nx.Graph, gpickle_file:str):\n",
    "    nx.write_gpickle(g, gpickle_file)\n",
    "\n",
    "def graph_load(gpickle_file:str):\n",
    "    return nx.read_gpickle(gpickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fundamental data (50 seconds)\n",
    "sent_list = my_read('../data/corpus/small_sent_reformed.txt')\n",
    "keywords = my_read('../data/corpus/entity.txt')\n",
    "word2idx = {word:i for i, word in enumerate(keywords)}\n",
    "occur_dict = occurance_load('../data/corpus/ent_occur.json')\n",
    "co_occur_list = co_occur_load('../data/corpus/ent_co_occur.txt')\n",
    "pair_graph = graph_load('../data/corpus/ent_pair.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence file with line number\n",
    "!grep -n '' ../data/corpus/small_sent.txt > ../data/corpus/small_sent_line.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occurance file\n",
    "# To run the code in the backend, use the gen_occur.py in the \"py\" folder\n",
    "p = MultiProcessing()\n",
    "occur_dict = p.run(lambda: Occurance('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt'), open('../data/corpus/small_sent_line.txt').readlines(), 8, occurance_post_operation)\n",
    "occurance_dump('../data/corpus/occur.json', occur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the file with line number\n",
    "!rm ../data/corpus/small_sent_line.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate co_occurance file\n",
    "co_occur_list = gen_co_occur(occur_dict, len(sent_list), word2idx)\n",
    "co_occur_dump('../data/corpus/ent_co_occur.txt', co_occur_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pair graph (about 5 minutes)\n",
    "pair_graph = build_graph(co_occur_list, keywords)\n",
    "graph_dump(pair_graph, '../data/corpus/ent_pair.gpickle')"
   ]
  },
  {
   "source": [
    "Play around in the below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = my_read('../data/test/co_occur_test.txt')\n",
    "test_data = [data.split(',') for data in test_data]\n",
    "test_dict = {data[0] : data[1:] for data in test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_dict = {central_kw : set() for central_kw in test_dict}\n",
    "for central_kw, kws in test_dict.items():\n",
    "    for kw in kws:\n",
    "        test_sent_dict[central_kw] |= occur_dict[kw]\n",
    "    test_sent_dict[central_kw] &= occur_dict[central_kw]\n",
    "\n",
    "for central_kw, sents in test_sent_dict.items():\n",
    "    content = [sent_list[i] for i in sents]\n",
    "    my_write('../data/temp/%s_wiki.txt' % central_kw.replace(' ', '_'), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = occur_dict['python'] & (occur_dict['java'] | occur_dict['ruby'])\n",
    "my_write('python_java_ruby.txt', [sent_list[i] for i in test_lines])"
   ]
  },
  {
   "source": [
    "Analyze the sentences with OLLIE, Stanford OpenIE or OpenIE5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openie_data = my_read('../data/temp/pl_wiki_ollie_triple.txt')\n",
    "# openie_data = my_read('pjr_ollie_triple.txt')\n",
    "# keywords = set(['data structure', 'binary tree', 'hash table', 'linked list'])\n",
    "keywords = set(['programming language', 'python', 'java', 'javascript', 'lua', 'scala', 'lisp', 'php', 'ruby', 'smalltalk'])\n",
    "# keywords = set(['python', 'java', 'ruby'])\n",
    "\n",
    "qualified_triples = []\n",
    "for data in openie_data:\n",
    "    if data:\n",
    "        arg1, rel, arg2 = data.split(';')\n",
    "        for kw in keywords:\n",
    "            if kw in arg1:\n",
    "                for kw in keywords:\n",
    "                    if kw in arg2:\n",
    "                        qualified_triples.append(data)\n",
    "                        break\n",
    "                break\n",
    "my_write('pl_wiki_ollie_triple_f.txt', qualified_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_idx = occur_dict['data structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur_set = {}\n",
    "for keyword, idx_set in occur_dict.items():\n",
    "    intersection = idx_set & data_structure_idx\n",
    "    if intersection:\n",
    "        co_occur_set[keyword] = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(co_occur_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_co_occur_list = sorted(co_occur_set.items(), key=lambda x: len(x[1]), reverse=True)[:100]\n",
    "sorted_co_occur_count = [(word, len(idx)) for word, idx in sorted_co_occur_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_co_occur_count[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'b-tree' in co_occur_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_list[co_occur_set['b-tree'][0]]\n",
    "temp_list = [sent_list[idx] for idx in co_occur_set['b-tree']]\n",
    "my_write('ds_bt_sent.txt', temp_list)"
   ]
  },
  {
   "source": [
    "Test of highly related pairs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_subgraph(g:nx.Graph, npmi_threshold:float, min_count:int):\n",
    "    return g.edge_subgraph([e[0] for e in pair_graph.edges.items() if e[1]['npmi'] > npmi_threshold and e[1]['c'] >= min_count])\n",
    "\n",
    "def find_dependency_path(sent:str, kw1:str, kw2:str):\n",
    "    doc = nlp(sent)\n",
    "    tokens = [t.text for t in doc]\n",
    "    try:\n",
    "        idx1 = tokens.index(kw1)\n",
    "        idx2 = tokens.index(kw2)\n",
    "    except:\n",
    "        return ''\n",
    "    branch = np.zeros(len(doc))\n",
    "    i = idx1\n",
    "    while branch[i] == 0:\n",
    "        branch[i] = 1\n",
    "        i = doc[i].head.i\n",
    "    i = idx2\n",
    "    while branch[i] == 0:\n",
    "        branch[i] = 2\n",
    "        i = doc[i].head.i\n",
    "    dep1 = []\n",
    "    j = idx1\n",
    "    while j != i:\n",
    "        dep1.append('i_%s' % doc[j].dep_)\n",
    "        j = doc[j].head.i\n",
    "    dep2 = []\n",
    "    j = idx2\n",
    "    while j != i:\n",
    "        dep2.append(doc[j].dep_)\n",
    "        j = doc[j].head.i\n",
    "    dep2.reverse()\n",
    "    if branch[idx2] == 1:\n",
    "        # kw2 is along the heads of kw1\n",
    "        return ' '.join(dep1)\n",
    "    elif i == idx1:\n",
    "        # kw1 is along the heads of kw2\n",
    "        return ' '.join(dep2)\n",
    "    else:\n",
    "        return ' '.join(dep1 + dep2)\n",
    "\n",
    "def mark_sent_in_html(sent:str, keywords:List[str], is_entity:bool=True):\n",
    "    reformed_sent = sent.split() if is_entity else sent_lemmatize(sent.replace('-', ' - '))\n",
    "    reformed_keywords = [[k] for k in keywords] if is_entity else [k.replace('-', ' - ').split() for k in keywords]\n",
    "    mask = np.zeros(len(reformed_sent), dtype=np.bool)\n",
    "    for k in reformed_keywords:\n",
    "        begin_idx = 0\n",
    "        while reformed_sent[begin_idx:].count(k[0]) > 0:\n",
    "            begin_idx = reformed_sent.index(k[0], begin_idx)\n",
    "            is_good = True\n",
    "            i = 0\n",
    "            for i in range(1, len(k)):\n",
    "                if begin_idx + i >= len(reformed_sent) or reformed_sent[begin_idx + i] != k[i]:\n",
    "                    is_good = False\n",
    "                    break\n",
    "            if is_good:\n",
    "                mask[begin_idx:begin_idx+i+1] = True\n",
    "            begin_idx += (i+1)\n",
    "    i = 0\n",
    "    insert_idx = 0\n",
    "    while i < len(mask):\n",
    "        if mask[i] and (i == 0 or mask[i-1] == False):\n",
    "            reformed_sent.insert(insert_idx, '<font style=\\\"color:red;\\\">')\n",
    "            insert_idx += 2\n",
    "            i += 1\n",
    "            while i < len(mask) and mask[i]:\n",
    "                i += 1\n",
    "                insert_idx += 1\n",
    "            reformed_sent.insert(insert_idx, '</font>')\n",
    "            insert_idx += 1\n",
    "        insert_idx += 1\n",
    "        i += 1\n",
    "    return ' '.join(reformed_sent)\n",
    "\n",
    "def gen_co_occur_report(report_file:str, g:nx.Graph, keyword:str, word2idx:Dict[str, int], keyword_list:List[str], occur_dict:Dict[str, set], sent_list:List[str], is_entity:bool=True, kw_dist_max:int=6):\n",
    "    neighbors = g.neighbors(word2idx[keyword])\n",
    "    related_kws = [keywords[idx] for idx in neighbors]\n",
    "    content = ['<a href=\\\"#%s__%s\\\">%s, %s</a><br>' % (keyword, kw, keyword, kw) for kw in related_kws]\n",
    "    for kw in related_kws:\n",
    "        content.append('<a id=\\\"%s__%s\\\"><h1>%s, %s</h1></a> ' % (keyword, kw, keyword, kw))\n",
    "        sents = [sent_list[i] for i in occur_dict[keyword] & occur_dict[kw]]\n",
    "        if is_entity:\n",
    "            sents = [sent.split() for sent in sents]\n",
    "            sents = [' '.join(sent) for sent in sents if sent.count(keyword) == 1 and sent.count(kw) == 1 and abs(sent.index(keyword) - sent.index(kw)) <= kw_dist_max]\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [keyword, kw], is_entity=is_entity) for sent in sents]\n",
    "    \n",
    "    my_write(report_file, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate highly related subgraph\n",
    "sub_g = get_subgraph(pair_graph, 0.3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = sub_g.neighbors(word2idx['data_structure'])\n",
    "related_kws = [keywords[idx] for idx in neighbors]\n",
    "print(related_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent_list[i] for i in (occur_dict['python'] & occur_dict['just-in-time compilation'])]\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g.edges[word2idx['python'], word2idx['just-in-time compilation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_co_occur_report('python_co_occur.html', sub_g, 'python', word2idx, keywords, occur_dict, sent_list, 6)"
   ]
  }
 ]
}