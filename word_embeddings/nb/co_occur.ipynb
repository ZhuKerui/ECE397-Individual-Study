{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd0947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Co-occurrence notebook\n",
    "+ This notebook is used for handling keyword co-occurrence related work"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import needed packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_write, MultiProcessing\n",
    "from tools.TextProcessing import sent_lemmatize\n",
    "from tools.DocProcessing.Occurrence import Occurrence, occurrence_dump, occurrence_load, occurrence_post_operation\n",
    "from tools.DocProcessing.CoOccurGraph import build_graph, graph_load, graph_dump, get_subgraph\n",
    "from tools.DocProcessing.CoOccurrence import gen_co_occur, co_occur_load, co_occur_dump\n",
    "import networkx as nx"
   ]
  },
  {
   "source": [
    "## Fundamental code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load fundamental data (50 seconds)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = my_read('../data/corpus/small_sent_reformed.txt')\n",
    "keyword_list = my_read('../data/corpus/entity.txt')\n",
    "word2idx_dict = {word:i for i, word in enumerate(keyword_list)}\n",
    "occur_dict = occurrence_load('../data/corpus/ent_occur.json')\n",
    "co_occur_list = co_occur_load('../data/corpus/ent_co_occur.txt')\n",
    "pair_graph = graph_load('../data/corpus/ent_pair.gpickle')"
   ]
  },
  {
   "source": [
    "### Generate occurrence dictionary if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence file with line number\n",
    "!grep -n '' ../data/corpus/small_sent_reformed.txt > ../data/corpus/small_sent_line.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate occurrence file\n",
    "# To run the code in the backend, use the gen_occur.py in the \"py\" folder\n",
    "p = MultiProcessing()\n",
    "occur_dict = p.run(lambda: Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt'), open('../data/corpus/small_sent_line.txt').readlines(), 8, occurrence_post_operation)\n",
    "occurrence_dump('../data/corpus/occur.json', occur_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the file with line number\n",
    "!rm ../data/corpus/small_sent_line.txt"
   ]
  },
  {
   "source": [
    "### Generate co-occurrence list if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate co_occurrence file\n",
    "co_occur_list = gen_co_occur(occur_dict, len(sent_list), word2idx_dict)\n",
    "co_occur_dump('../data/corpus/ent_co_occur.txt', co_occur_list)"
   ]
  },
  {
   "source": [
    "### Generate co-occurrence graph if needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pair graph (about 5 minutes)\n",
    "pair_graph = build_graph(co_occur_list, keyword_list)\n",
    "graph_dump(pair_graph, '../data/corpus/ent_pair.gpickle')"
   ]
  },
  {
   "source": [
    "## Play around in the below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Test of highly related pairs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def find_highly_related_keyword(g:nx.Graph, keyword:str, word2idx_dict:Dict[str, int], keyword_list:List[str]):\n",
    "    neighbors = g.neighbors(word2idx_dict[keyword])\n",
    "    related_kws = [keyword_list[idx] for idx in neighbors]\n",
    "    print(related_kws)\n",
    "\n",
    "def mark_sent_in_html(sent:str, keyword_list:List[str], is_entity:bool=True):\n",
    "    reformed_sent = sent.split() if is_entity else sent_lemmatize(sent.replace('-', ' - '))\n",
    "    reformed_keywords = [[k] for k in keyword_list] if is_entity else [k.replace('-', ' - ').split() for k in keyword_list]\n",
    "    mask = np.zeros(len(reformed_sent), dtype=np.bool)\n",
    "    for k in reformed_keywords:\n",
    "        begin_idx = 0\n",
    "        while reformed_sent[begin_idx:].count(k[0]) > 0:\n",
    "            begin_idx = reformed_sent.index(k[0], begin_idx)\n",
    "            is_good = True\n",
    "            i = 0\n",
    "            for i in range(1, len(k)):\n",
    "                if begin_idx + i >= len(reformed_sent) or reformed_sent[begin_idx + i] != k[i]:\n",
    "                    is_good = False\n",
    "                    break\n",
    "            if is_good:\n",
    "                mask[begin_idx:begin_idx+i+1] = True\n",
    "            begin_idx += (i+1)\n",
    "    i = 0\n",
    "    insert_idx = 0\n",
    "    while i < len(mask):\n",
    "        if mask[i] and (i == 0 or mask[i-1] == False):\n",
    "            reformed_sent.insert(insert_idx, '<font style=\\\"color:red;\\\">')\n",
    "            insert_idx += 2\n",
    "            i += 1\n",
    "            while i < len(mask) and mask[i]:\n",
    "                i += 1\n",
    "                insert_idx += 1\n",
    "            reformed_sent.insert(insert_idx, '</font>')\n",
    "            insert_idx += 1\n",
    "        insert_idx += 1\n",
    "        i += 1\n",
    "    return ' '.join(reformed_sent)\n",
    "\n",
    "def gen_co_occur_report(report_file:str, g:nx.Graph, keyword:str, word2idx_dict:Dict[str, int], keyword_list:List[str], occur_dict:Dict[str, set], sent_list:List[str], is_entity:bool=True, kw_dist_max:int=6):\n",
    "    neighbors = g.neighbors(word2idx_dict[keyword])\n",
    "    related_kws = [keyword_list[idx] for idx in neighbors]\n",
    "    content = ['<a href=\\\"#%s__%s\\\">%s, %s</a><br>' % (keyword, kw, keyword, kw) for kw in related_kws]\n",
    "    for kw in related_kws:\n",
    "        content.append('<a id=\\\"%s__%s\\\"><h1>%s, %s</h1></a> ' % (keyword, kw, keyword, kw))\n",
    "        sents = [sent_list[i] for i in occur_dict[keyword] & occur_dict[kw]]\n",
    "        if is_entity:\n",
    "            sents = [sent.split() for sent in sents]\n",
    "            sents = [' '.join(sent) for sent in sents if sent.count(keyword) == 1 and sent.count(kw) == 1 and abs(sent.index(keyword) - sent.index(kw)) <= kw_dist_max]\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [keyword, kw], is_entity=is_entity) for sent in sents]\n",
    "    \n",
    "    my_write(report_file, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate highly related subgraph\n",
    "sub_g = get_subgraph(pair_graph, 0.3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_test_pairs = [('python', 'java'), ('stack', 'queue')]\n",
    "\n",
    "title = []\n",
    "content = []\n",
    "for pair in neighbor_test_pairs:\n",
    "    mid_set = set(sub_g.neighbors(word2idx_dict[pair[0]])) & set(sub_g.neighbors(word2idx_dict[pair[1]]))\n",
    "    if not mid_set:\n",
    "        print('%s and %s fail in one hop relation' % (pair[0], pair[1]))\n",
    "        continue\n",
    "    title.append('<a href=\\\"#%s__%s\\\">%s, %s</a><br>' % (pair[0], pair[1], pair[0], pair[1]))\n",
    "    content.append('<a id=\\\"%s__%s\\\"></a> <h1>%s, %s</h1>' % (pair[0], pair[1], pair[0], pair[1]))\n",
    "    for mid in mid_set:\n",
    "        mid_text = keyword_list[mid]\n",
    "        content.append('<h2>%s, %s</h2>' % (pair[0], mid_text))\n",
    "        temp_sents = [sent_list[i].split() for i in occur_dict[pair[0]] & occur_dict[mid_text]]\n",
    "        temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[0]) == 1 and sent.count(mid_text) == 1 and abs(sent.index(pair[0]) - sent.index(mid_text)) <= 6]\n",
    "        mark_sents = []\n",
    "        for sent in temp_sents:\n",
    "            doc = nlp(sent)\n",
    "            tokens = [word.text for word in doc]\n",
    "            try:\n",
    "                idx1, idx2 = tokens.index(pair[0]), tokens.index(mid_text)\n",
    "            except:\n",
    "                continue\n",
    "            if doc[idx1].dep_ == 'nsubj' or doc[idx1].dep_ == 'dobj' or doc[idx2].dep_ == 'nsubj' or doc[idx2].dep_ == 'dobj':\n",
    "                mark_sents.append(sent)\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[0], mid_text]) for sent in mark_sents]\n",
    "        \n",
    "        content.append('<h2>%s, %s</h2>' % (pair[1], mid_text))\n",
    "        temp_sents = [sent_list[i].split() for i in occur_dict[pair[1]] & occur_dict[mid_text]]\n",
    "        temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[1]) == 1 and sent.count(mid_text) == 1 and abs(sent.index(pair[1]) - sent.index(mid_text)) <= 6]\n",
    "        mark_sents = []\n",
    "        for sent in temp_sents:\n",
    "            doc = nlp(sent)\n",
    "            tokens = [word.text for word in doc]\n",
    "            try:\n",
    "                idx1, idx2 = tokens.index(pair[1]), tokens.index(mid_text)\n",
    "            except:\n",
    "                continue\n",
    "            if doc[idx1].dep_ == 'nsubj' or doc[idx1].dep_ == 'dobj' or doc[idx2].dep_ == 'nsubj' or doc[idx2].dep_ == 'dobj':\n",
    "                mark_sents.append(sent)\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[1], mid_text]) for sent in mark_sents]\n",
    "\n",
    "        # content.append('<h2>%s, %s, %s</h2>' % (pair[0], mid_text, pair[1]))\n",
    "        # temp_sents = [sent_list[i].split() for i in occur_dict[pair[0]] & occur_dict[mid_text] & occur_dict[pair[1]]]\n",
    "        # temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[0]) == 1 and sent.count(pair[1]) == 1 and sent.count(mid_text) == 1]\n",
    "        # content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[0], mid_text, pair[1]]) for sent in temp_sents]\n",
    "my_write('overlap_test.html', title + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g.edges[word2idx_dict['python'], word2idx_dict['just-in-time compilation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_co_occur_report('ds_co_occur.html', sub_g, 'data_structure', word2idx_dict, keyword_list, occur_dict, sent_list, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'i_dobj prep pobj'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "find_dependency_path('efficient_point location in the sinr diagram , i.e. , building a data_structure to determine , for a query_point , whether any transmitter is heard there , and if so , which one , has been recently investigated .', 'data_structure', 'query_point')"
   ]
  },
  {
   "source": [
    "### Analyze the sentences with OLLIE, Stanford OpenIE or OpenIE5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = my_read('../data/test/co_occur_test.txt')\n",
    "# test_data = [data.split(',') for data in test_data]\n",
    "# test_dict = {data[0] : data[1:] for data in test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sent_dict = {central_kw : set() for central_kw in test_dict}\n",
    "# for central_kw, kws in test_dict.items():\n",
    "#     for kw in kws:\n",
    "#         test_sent_dict[central_kw] |= occur_dict[kw]\n",
    "#     test_sent_dict[central_kw] &= occur_dict[central_kw]\n",
    "\n",
    "# for central_kw, sents in test_sent_dict.items():\n",
    "#     content = [sent_list[i] for i in sents]\n",
    "#     my_write('../data/temp/%s_wiki.txt' % central_kw.replace(' ', '_'), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_lines = occur_dict['python'] & (occur_dict['java'] | occur_dict['ruby'])\n",
    "# my_write('python_java_ruby.txt', [sent_list[i] for i in test_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openie_data = my_read('../data/temp/pl_wiki_ollie_triple.txt')\n",
    "# # openie_data = my_read('pjr_ollie_triple.txt')\n",
    "# # keywords = set(['data structure', 'binary tree', 'hash table', 'linked list'])\n",
    "# keywords = set(['programming language', 'python', 'java', 'javascript', 'lua', 'scala', 'lisp', 'php', 'ruby', 'smalltalk'])\n",
    "# # keywords = set(['python', 'java', 'ruby'])\n",
    "\n",
    "# qualified_triples = []\n",
    "# for data in openie_data:\n",
    "#     if data:\n",
    "#         arg1, rel, arg2 = data.split(';')\n",
    "#         for kw in keywords:\n",
    "#             if kw in arg1:\n",
    "#                 for kw in keywords:\n",
    "#                     if kw in arg2:\n",
    "#                         qualified_triples.append(data)\n",
    "#                         break\n",
    "#                 break\n",
    "# my_write('pl_wiki_ollie_triple_f.txt', qualified_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_structure_idx = occur_dict['data structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(co_occur_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_occur_set = {}\n",
    "# for keyword, idx_set in occur_dict.items():\n",
    "#     intersection = idx_set & data_structure_idx\n",
    "#     if intersection:\n",
    "#         co_occur_set[keyword] = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_co_occur_list = sorted(co_occur_set.items(), key=lambda x: len(x[1]), reverse=True)[:100]\n",
    "# sorted_co_occur_count = [(word, len(idx)) for word, idx in sorted_co_occur_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_co_occur_count[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'b-tree' in co_occur_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sent_list[co_occur_set['b-tree'][0]]\n",
    "# temp_list = [sent_list[idx] for idx in co_occur_set['b-tree']]\n",
    "# my_write('ds_bt_sent.txt', temp_list)"
   ]
  }
 ]
}